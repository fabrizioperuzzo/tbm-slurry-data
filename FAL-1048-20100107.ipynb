{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from images2gif import writeGif\n",
    "import IPython.display as IPdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per matrice correlazione\n",
    "import seaborn as sns\n",
    "# Scipy helper functions\n",
    "from scipy.stats import percentileofscore\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\fpe\\\\JUPYTER\\\\datimacchina\\\\DATA\\\\1048'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()+'\\\\DATA\\\\1048'\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GEODATA_S-1048_RAW_20170821_28_-_week34.xls',\n",
       " 'GEODATA_S-1048_RAW_20170828_0904_-_week35.xls',\n",
       " 'GEODATA_S-1048_RAW_20170904_11_-_week36.xls',\n",
       " 'GEODATA_S-1048_RAW_20170911_18_-_week37.xls',\n",
       " 'GEODATA_S-1048_RAW_20170918_25_-_week38.xls',\n",
       " 'GEODATA_S-1048_RAW_20170925_1002_-_week39.xls',\n",
       " 'GEODATA_S-1048_RAW_20171002_09_-_week40.xls',\n",
       " 'GEODATA_S-1048_RAW_20171009_16_-_week41.xls',\n",
       " 'GEODATA_S-1048_RAW_20171016_23_-_week42.xls',\n",
       " 'GEODATA_S-1048_RAW_20171023_30_-_week43.xls',\n",
       " 'GEODATA_S-1048_RAW_20171030_1106_-_week44.xls',\n",
       " 'GEODATA_S-1048_RAW_20171106_13_-_week45.xls',\n",
       " 'GEODATA_S-1048_RAW_20171113_20_-_week46.xls',\n",
       " 'GEODATA_S-1048_RAW_20171120_27_-_week47.xls',\n",
       " 'GEODATA_S-1048_RAW_20171127_1204_-_week48.xls',\n",
       " 'GEODATA_S-1048_RAW_20171204_11_-_week49.xls',\n",
       " 'GEODATA_S-1048_RAW_20171211_18_-_week50.xls',\n",
       " 'GEODATA_S-1048_RAW_20171218_25_-_week51.xls',\n",
       " 'GEODATA_S-1048_RAW_20171225_20180101_-_week52.xls',\n",
       " 'GEODATA_S-1048_RAW_20180101_08_-_week01.xls',\n",
       " 'GEODATA_S-1048_RAW_20180108_15_-_week02.xls',\n",
       " 'GEODATA_S-1048_RAW_20180115_22_-_week03.xls',\n",
       " 'GEODATA_S-1048_RAW_20180122_29_-_week04.xls',\n",
       " 'GEODATA_S-1048_RAW_20180129_0205_-_week05.xls',\n",
       " 'GEODATA_S-1048_RAW_20180205_12_-_week06.xls',\n",
       " 'GEODATA_S-1048_RAW_20180212_19_-_week07.xls',\n",
       " 'GEODATA_S-1048_RAW_20180219_26_-_week08.xls',\n",
       " 'GEODATA_S-1048_RAW_20180226_0305_-_week09.xls',\n",
       " 'GEODATA_S-1048_RAW_20180305_12_-_week10.xls',\n",
       " 'GEODATA_S-1048_RAW_20180312_19_-_week11.xls',\n",
       " 'GEODATA_S-1048_RAW_20180319_26_-_week12.xls',\n",
       " 'GEODATA_S-1048_RAW_20180326_0402_-_week13.xls',\n",
       " 'GEODATA_S-1048_RAW_20180402_09_-_week14.xls',\n",
       " 'GEODATA_S-1048_RAW_20180409_16_-_week15.xls',\n",
       " 'GEODATA_S-1048_RAW_20180416_23_-_week16.xls',\n",
       " 'GEODATA_S-1048_RAW_20180423_30_-_week17.xls',\n",
       " 'GEODATA_S-1048_RAW_20180430_0507_-_week18.xls',\n",
       " 'GEODATA_S-1048_RAW_20180507_14_-_week19.xls',\n",
       " 'GEODATA_S-1048_RAW_20180514_21_-_week20.xls',\n",
       " 'GEODATA_S-1048_RAW_20180521_28_-_week21.xls',\n",
       " 'GEODATA_S-1048_RAW_20180528_0604_-_week22.xls',\n",
       " 'GEODATA_S-1048_RAW_20180604_11_-_week23.xls',\n",
       " 'GEODATA_S-1048_RAW_20180611_18_-_week24.xls',\n",
       " 'GEODATA_S-1048_RAW_20180618_25_-_week25.xls',\n",
       " 'GEODATA_S-1048_RAW_20180625_0702_-_week26.xls',\n",
       " 'GEODATA_S-1048_RAW_20180702_09_-_week27.xls',\n",
       " 'GEODATA_S-1048_RAW_20180709_16_-_week28.xls',\n",
       " 'GEODATA_S-1048_RAW_20180716_23_-_week29.xls',\n",
       " 'GEODATA_S-1048_RAW_20180723_30_-_week30.xls',\n",
       " 'GEODATA_S-1048_RAW_20180730_0806_-_week31.xls',\n",
       " 'GEODATA_S-1048_RAW_20180806_13_-_week32.xls',\n",
       " 'GEODATA_S-1048_RAW_20180813_20_-_week33.xls',\n",
       " 'GEODATA_S-1048_RAW_20180820_27_-_week34.xls',\n",
       " 'GEODATA_S-1048_RAW_20180827_0903_-_week35.xls',\n",
       " 'GEODATA_S-1048_RAW_20180903_10_-_week36.xls',\n",
       " 'GEODATA_S-1048_RAW_20180910_17_-_week37.xls',\n",
       " 'GEODATA_S-1048_RAW_20180917_24_-_week38.xls',\n",
       " 'GEODATA_S-1048_RAW_20180924_1001_-_week39.xls',\n",
       " 'GEODATA_S-1048_RAW_20181001_08_-_week40.xls',\n",
       " 'GEODATA_S-1048_RAW_20181008_15_-_week41.xls',\n",
       " 'GEODATA_S-1048_RAW_20181015_22_-_week42.xls',\n",
       " 'GEODATA_S-1048_RAW_20181022_29_-_week43.xls',\n",
       " 'GEODATA_S-1048_RAW_20181029_1105_-_week44.xls',\n",
       " 'GEODATA_S-1048_RAW_20181105_12_-_week45.xls',\n",
       " 'GEODATA_S-1048_RAW_20181112_19_-_week46.xls',\n",
       " 'GEODATA_S-1048_RAW_20181119_26_-_week47.xls',\n",
       " 'GEODATA_S-1048_RAW_20181126_1203_-_week48.xls',\n",
       " 'GEODATA_S-1048_RAW_20181203_10_-_week49.xls',\n",
       " 'GEODATA_S-1048_RAW_20181210_17_-_week50.xls',\n",
       " 'GEODATA_S-1048_RAW_20181217_24_-_week51.xls',\n",
       " 'GEODATA_S-1048_RAW_20181224_31_-_week52.xls',\n",
       " 'GEODATA_S-1048_RAW_20181231_20190107_-_week01.xls',\n",
       " 'GEODATA_S-1048_RAW_20190107_14_-_week02.xls',\n",
       " 'GEODATA_S-1048_RAW_20190114_21_-_week03.xls',\n",
       " 'GEODATA_S-1048_RAW_20190121_28_-_week04.xls',\n",
       " 'GEODATA_S-1048_RAW_20190128_0204_-_week05.xls',\n",
       " 'GEODATA_S-1048_RAW_20190204_11_-_week06.xls',\n",
       " 'GEODATA_S-1048_RAW_20190211_18_-_week07.xls',\n",
       " 'GEODATA_S-1048_RAW_20190218_25_-_week08.xls',\n",
       " 'GEODATA_S-1048_RAW_20190225_0304_-_week09.xls',\n",
       " 'GEODATA_S-1048_RAW_20190304_0311_-_week10.xls',\n",
       " 'GEODATA_S-1048_RAW_20190311_0318_-_week11.xls',\n",
       " 'GEODATA_S-1048_RAW_20190318_0325_-_week12.xls',\n",
       " 'GEODATA_S-1048_RAW_20190325_0401_-_week13.xls',\n",
       " 'GEODATA_S-1048_RAW_20190401_0408_-_week14.xls',\n",
       " 'GEODATA_S-1048_RAW_20190408_0415_-_week15.xls',\n",
       " 'GEODATA_S-1048_RAW_20190415_0422_-_week16.xls',\n",
       " 'GEODATA_S-1048_RAW_20190422_0429_-_week17.xls',\n",
       " 'GEODATA_S-1048_RAW_20190429_0506_-_week18.xls',\n",
       " 'GEODATA_S-1048_RAW_20190506_0513_-_week19.xls',\n",
       " 'GEODATA_S-1048_RAW_20190513_0520_-_week20.xls',\n",
       " 'GEODATA_S-1048_RAW_20190520_0527_-_week21.xls',\n",
       " 'GEODATA_S-1048_RAW_20190527_0603_-_week22.xls',\n",
       " 'GEODATA_S-1048_RAW_20190603_0610_-_week23.xls',\n",
       " 'GEODATA_S-1048_RAW_20190617_0624_-_week25.xls',\n",
       " 'GEODATA_S-1048_RAW_20190624_0701_-_week26.xls',\n",
       " 'GEODATA_S-1048_RAW_20190701_0708_-_week27.xls',\n",
       " 'GEODATA_S-1048_RAW_20190907_0916_-_week_37.xls']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "filespath = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.xls' in file:\n",
    "            files.append(file)\n",
    "            filespath.append(os.path.join(r, file))\n",
    "            \n",
    "# since there is only one file I need only the file name\n",
    "os.path.join(r, files[0])\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    os.rename(os.path.join(path, file), os.path.join(path, file.replace(' ', '_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "filespath = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.xls' in file:\n",
    "            files.append(file)\n",
    "            filespath.append(os.path.join(r, file))\n",
    "            \n",
    "# since there is only one file I need only the file name\n",
    "os.path.join(r, files[0])\n",
    "files[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls=pd.DataFrame({})\n",
    "for file in files:\n",
    "    try:\n",
    "        xlstemp = pd.read_excel(os.path.join(r, file), 'Sheet0')\n",
    "        #xlstemp['CPTname']=file.replace('.xls','')\n",
    "        xls = xls.append(xlstemp, ignore_index = True, sort=True)\n",
    "    except:\n",
    "        print('A file was not loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'GEODATA_S-1048_RAW_20181015_22_-_week42.xls'\n",
    "xls = pd.read_excel(os.path.join(r, file), 'Sheet0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel(os.path.join(r, 'GEODATA_S-1048_RAW_20170828_0904_-_week35.xls'), 'Sheet0')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_str = '%Y-%m-%d'\n",
    "date = datetime.datetime.strptime('2019-09-04', format_str)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxls['datestamp']=pd.to_datetime(dfxls.Date)\n",
    "dfxls['timestamp']=dfxls.datestamp.apply(lambda x: int(time.mktime(x.timetuple())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxls.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxls[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxls.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxls.iloc[:,[0,4]].plot()\n",
    "dfxls.iloc[:,[0,8]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxls.filter(regex='level').columns.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cerco la posizione del fill level\n",
    "\n",
    "pos_filllev=int(dfxls.columns.get_loc(dfxls.filter(regex='level').columns.tolist()[0]))\n",
    "pos_filllev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo una lista con le posizioni dei sensori delle camere\n",
    "list_chamb = []\n",
    "for i in dfxls.filter(regex='Pressure excavation chamber'):\n",
    "    list_chamb.append(dfxls.columns.get_loc(i))\n",
    "print(list_chamb)\n",
    "\n",
    "list_col = [0]\n",
    "list_col.extend(list_chamb)\n",
    "list_col.append(pos_filllev)\n",
    "print(list_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dati = -1\n",
    "\n",
    "df = dfxls.iloc[:num_dati,list_col]\n",
    "n_row = df.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# *************************************************\n",
    "\n",
    "n_sensori = 8\n",
    "list_x = []\n",
    "list_z = []\n",
    "for i in np.arange(0,n_sensori,1):\n",
    "    x = 1 * np.sin(2*np.pi/8*i)\n",
    "    list_x.append(x)\n",
    "    z = 1 * np.cos(2*np.pi/8*i)\n",
    "    list_z.append(z)\n",
    "\n",
    "    \n",
    "#  ************************************************\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "shutil.rmtree('images', ignore_errors=True)\n",
    "save_folder = 'images/gif'\n",
    "gif_filename = '01'\n",
    "\n",
    "working_folder = '{}/{}'.format(save_folder, gif_filename)\n",
    "\n",
    "if not os.path.exists(working_folder):\n",
    "    os.makedirs(working_folder)\n",
    "\n",
    "fontdict1={'fontsize': 18,\n",
    "          'fontweight':0.1,\n",
    "          'horizontalalignment': 'center'}\n",
    "\n",
    "fontdict2 = {'family': 'serif',\n",
    "        'color':  'white',\n",
    "        'weight': 'normal',\n",
    "        'size': 16,\n",
    "        }\n",
    "\n",
    "for e in np.arange(0,n_row,1):\n",
    "    \n",
    "    if e%10==0:    # creo immagine ogni 10 print\n",
    "        try:\n",
    "            ax.clear()\n",
    "            ax1.clear()\n",
    "        except:\n",
    "            pass\n",
    "        fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "        ax = fig.add_subplot(121, projection='3d')\n",
    "        ax1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\") \n",
    "        ax.set_zlabel(\"z\")\n",
    "\n",
    "        ax.view_init(45, -45)\n",
    "\n",
    "        ax.set_xlim3d(-1,1)\n",
    "        ax.set_ylim3d(1.5,0) # same as saying to invert it \n",
    "        ax.set_zlim3d(-1,1) \n",
    "\n",
    "        #######      *********\n",
    "\n",
    "        ax1.set_xlim3d(-1,1)\n",
    "        ax1.set_ylim3d(-1,1) # same as saying to invert it \n",
    "        ax1.set_zlim3d(0,3) \n",
    "\n",
    "\n",
    "\n",
    "        ax.set_xlabel('x'),ax.set_ylabel('Pressure [bar]'),ax.set_zlabel('y')\n",
    "\n",
    "        n_row = df.shape[0]\n",
    "\n",
    "        for i in np.arange(0,n_sensori,1):\n",
    "\n",
    "            x_ = list_x[i]\n",
    "            z_ = list_z[i]\n",
    "            y_ = df.iloc[e,i+1]-1   # tolgo 1bar per renderle omogenee\n",
    "            z1_ = df.iloc[e,-1]*3   # tmopltiplico *3\n",
    "\n",
    "\n",
    "\n",
    "            ax.bar3d(x_, y_, z_, dx=0.2, dz=0.2, dy=(0-y_),\n",
    "                    color='deepskyblue')\n",
    "\n",
    "\n",
    "        ax1.bar3d(0, 0, z1_, dx=0.5, dy=0.5, dz=(0-z1_),\n",
    "                    color='r')\n",
    "\n",
    "        ax.text2D(0.05, 0.99, str(df.iloc[e,0]), transform=ax.transAxes, fontdict = fontdict2)\n",
    "\n",
    "        ax.text2D(0.05, 0, str('Pressure chamber'), transform=ax.transAxes, fontdict = fontdict2)\n",
    "        ax1.text2D(2, 0, str('Fill Level'), transform=ax.transAxes, fontdict = fontdict2)\n",
    "    \n",
    "    \n",
    "        plt.savefig('{}/{}/img{:03d}.png'.format(save_folder, gif_filename, e))    \n",
    "        \n",
    "        \n",
    "    \n",
    "# don't display the static plot...\n",
    "plt.close()\n",
    "\n",
    "# load all the static images into a list then save as an animated gif\n",
    "gif_filepath = '{}/{}.gif'.format(save_folder, gif_filename)\n",
    "\n",
    "images = []\n",
    "images = [Image.open(image) for image in glob.glob('{}/*.png'.format(working_folder))]\n",
    "gif = images[0]\n",
    "gif.info['duration'] = 75 #milliseconds per frame\n",
    "gif.info['loop'] = 0 #how many times to loop (0=infinite)\n",
    "append_images=[]\n",
    "gif.save(fp=gif_filepath, format='gif', save_all=True, append_images=images[1:])\n",
    "IPdisplay.Image(url=gif_filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxls.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfg = dfxls.iloc[:,[2,4,6,9,10,11,19,20,22,26,27,30,32,33,34,37,50,51]]\n",
    "dfg = dfxls.iloc[:,[2,4,6,9,10,11,19,20,22,26,27,30,32]]\n",
    "dfg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient\n",
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .6), xycoords=ax.transAxes,\n",
    "               size = 20)\n",
    "    \n",
    "plt.style.use('seaborn-pastel')   \n",
    "sns.set_style(\"white\")\n",
    "    \n",
    "cmap = sns.cubehelix_palette(light=1, dark = 0.1,\n",
    "                             hue = 0.5, as_cmap=True)\n",
    "\n",
    "sns.set_context(font_scale=2)\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "sns.set(rc={'figure.figsize':(30,30)})\n",
    "\n",
    "\n",
    "# Pair grid set up\n",
    "g = sns.PairGrid(dfg)\n",
    "\n",
    "# Scatter plot on the upper triangle\n",
    "g.map_upper(plt.scatter, s=10, color = 'b')\n",
    "\n",
    "# Distribution on the diagonal\n",
    "g.map_diag(sns.distplot, kde=False, color = 'red')\n",
    "\n",
    "# Density Plot and Correlation coefficients on the lower triangle\n",
    "g.map_lower(sns.kdeplot, cmap = cmap)\n",
    "g.map_lower(corrfunc);\n",
    "\n",
    "plt.savefig('{}/{}/img{:03d}.png'.format(save_folder, gif_filename, 'corr_matrix'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg=dfxls.filter(regex='(Pressure excavation chamber|Fill level)')\n",
    "\n",
    "col_ls = dfg.columns.tolist()\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Pressure excavation chamber \",\"Pr_Ex_Ch_\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Fill level working chamber\",\"Fill_Lev_WCh\"),col_ls))\n",
    "dfg.columns = col_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxls.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg=dfxls.filter(regex='(Pressure excavation chamber|Fill level)')\n",
    "\n",
    "col_ls = dfg.columns.tolist()\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Pressure excavation chamber \",\"Pr_Ex_Ch_\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Fill level working chamber\",\"Fill_Lev_WCh\"),col_ls))\n",
    "dfg.columns = col_ls\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .6), xycoords=ax.transAxes,\n",
    "               size = 20)\n",
    "    \n",
    "plt.style.use('seaborn-pastel')   \n",
    "sns.set_style(\"white\")\n",
    "    \n",
    "cmap = sns.cubehelix_palette(light=1, dark = 0.1,\n",
    "                             hue = 0.5, as_cmap=True)\n",
    "\n",
    "sns.set_context(font_scale=2)\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "sns.set(rc={'figure.figsize':(30,30)})\n",
    "\n",
    "\n",
    "# Pair grid set up\n",
    "g = sns.PairGrid(dfg)\n",
    "\n",
    "# Scatter plot on the upper triangle\n",
    "g.map_upper(plt.scatter, s=10, color = 'b')\n",
    "\n",
    "# Distribution on the diagonal\n",
    "g.map_diag(sns.distplot, kde=False, color = 'red')\n",
    "\n",
    "# Density Plot and Correlation coefficients on the lower triangle\n",
    "g.map_lower(sns.kdeplot, cmap = cmap)\n",
    "g.map_lower(corrfunc);\n",
    "\n",
    "plt.savefig('{}/{}/img_{}.png'.format(save_folder, gif_filename, 'corr_matrix_press'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .6), xycoords=ax.transAxes,\n",
    "               size = 20)\n",
    "\n",
    "def corrgraph(dfg, nomefile):\n",
    "    # Function to create graph correlation matrix\n",
    "\n",
    "    plt.style.use('seaborn-pastel')   \n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    cmap = sns.cubehelix_palette(light=1, dark = 0.1,\n",
    "                                 hue = 0.5, as_cmap=True)\n",
    "\n",
    "    sns.set_context(font_scale=2)\n",
    "\n",
    "    fig = plt.figure(figsize=(30,30))\n",
    "    sns.set(rc={'figure.figsize':(30,30)})\n",
    "\n",
    "\n",
    "    # Pair grid set up\n",
    "    g = sns.PairGrid(dfg)\n",
    "\n",
    "    # Scatter plot on the upper triangle\n",
    "    g.map_upper(plt.scatter, s=10, color = 'b')\n",
    "\n",
    "    # Distribution on the diagonal\n",
    "    g.map_diag(sns.distplot, kde=False, color = 'red')\n",
    "\n",
    "    # Density Plot and Correlation coefficients on the lower triangle\n",
    "    g.map_lower(sns.kdeplot, cmap = cmap)\n",
    "    g.map_lower(corrfunc);\n",
    "\n",
    "    plt.savefig('{}/{}/img_{}.png'.format(save_folder, gif_filename, nomefile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Seleziono colonne utili\n",
    "\n",
    "dfg=dfxls.filter(regex='(Pressure excavation chamber 5|Pressure excavation chamber 8|Fill level|grout injection pressure|\\\n",
    "grout injection amount|grout injection flow|Air Flow in Normal)')\n",
    "\n",
    "# Semplifico il nome delle colonne\n",
    "col_ls = dfg.columns.tolist()\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Pressure excavation chamber \",\"Pr_Ex_Ch_\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Fill level working chamber\",\"Fill_Lev_WCh\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Air Flow in Normal m3/min\",\"Air_Flow_in\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\" grout injection \",\"_Gr_in_\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"pressure\",\"Pr\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Line \",\"L_\"),col_ls))\n",
    "dfg.columns = col_ls\n",
    "\n",
    "# Verifico i risultati dell'operazione\n",
    "col_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrgraph(dfg, 'corr_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Seleziono colonne utili\n",
    "\n",
    "dfg=dfxls.filter(regex='(Pressure excavation chamber 5|Pressure excavation chamber 8|Fill level|grout injection pressure|\\\n",
    "Line 1 grout injection amount|Line 1 grout injection flow)')\n",
    "\n",
    "# Semplifico il nome delle colonne\n",
    "col_ls = dfg.columns.tolist()\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Pressure excavation chamber \",\"Pr_Ex_Ch_\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Fill level working chamber\",\"Fill_Lev_WCh\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Air Flow in Normal m3/min\",\"Air_Flow_in\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Line 1 grout injection \",\"L1_Gr_in_\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"pressure\",\"Pr\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Line \",\"L_\"),col_ls))\n",
    "dfg.columns = col_ls\n",
    "\n",
    "# Verifico i risultati dell'operazione\n",
    "col_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrgraph(dfg, 'corr_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import statsmodels as sm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "#matplotlib.style.use('ggplot')\n",
    "\n",
    "# Create models from data\n",
    "def best_fit_distribution(data, bins=10, ax=None):\n",
    "    \"\"\"Model data by finding best fit distribution to data\"\"\"\n",
    "    # Get histogram of original data\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "\n",
    "    # Distributions to check\n",
    "    DISTRIBUTIONS = [        \n",
    "        st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,\n",
    "        st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,\n",
    "        st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,\n",
    "        st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,\n",
    "        st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,\n",
    "        st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,\n",
    "        st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,\n",
    "        st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,\n",
    "        st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,\n",
    "        st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy]\n",
    "\n",
    "\n",
    "    # Best holders\n",
    "    best_distribution = st.norm\n",
    "    best_params = (0.0, 1.0)\n",
    "    best_sse = np.inf\n",
    "\n",
    "    # Estimate distribution parameters from data\n",
    "    for distribution in DISTRIBUTIONS:\n",
    "\n",
    "        # Try to fit the distribution\n",
    "        try:\n",
    "            # Ignore warnings from data that can't be fit\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                # fit dist to data\n",
    "                params = distribution.fit(data)\n",
    "\n",
    "                # Separate parts of parameters\n",
    "                arg = params[:-2]\n",
    "                loc = params[-2]\n",
    "                scale = params[-1]\n",
    "\n",
    "                # Calculate fitted PDF and error with fit in distribution\n",
    "                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n",
    "                sse = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "                # if axis pass in add to plot\n",
    "                try:\n",
    "                    if ax:\n",
    "                        pd.Series(pdf, x).plot(ax=ax)\n",
    "                    end\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # identify if this distribution is better\n",
    "                if best_sse > sse > 0:\n",
    "                    best_distribution = distribution\n",
    "                    best_params = params\n",
    "                    best_sse = sse\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return (best_distribution.name, best_params)\n",
    "\n",
    "def make_pdf(dist, params, size=10000):\n",
    "    \"\"\"Generate distributions's Probability Distribution Function \"\"\"\n",
    "\n",
    "    # Separate parts of parameters\n",
    "    arg = params[:-2]\n",
    "    loc = params[-2]\n",
    "    scale = params[-1]\n",
    "\n",
    "    # Get sane start and end points of distribution\n",
    "    start = dist.ppf(0.01, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.01, loc=loc, scale=scale)\n",
    "    end = dist.ppf(0.99, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.99, loc=loc, scale=scale)\n",
    "\n",
    "    # Build PDF and turn into pandas Series\n",
    "    x = np.linspace(start, end, size)\n",
    "    y = dist.pdf(x, loc=loc, scale=scale, *arg)\n",
    "    pdf = pd.Series(y, x)\n",
    "\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###     CREATE LAST DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Seleziono colonne utili\n",
    "\n",
    "dfg=dfxls.filter(regex='(Pressure excavation chamber 5|Pressure excavation chamber 8|Fill level|grout injection pressure|\\\n",
    "Line 1 grout injection amount|Line 1 grout injection flow)').dropna()\n",
    "\n",
    "# Semplifico il nome delle colonne\n",
    "col_ls = dfg.columns.tolist()\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Pressure excavation chamber \",\"Pr_Ex_Ch_\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Fill level working chamber\",\"Fill_Lev_WCh\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Air Flow in Normal m3/min\",\"Air_Flow_in\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Line 1 grout injection \",\"L1_Gr_in_\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"pressure\",\"Pr\"),col_ls))\n",
    "col_ls = list(map(lambda x: str.replace(x,\"Line \",\"L_\"),col_ls))\n",
    "dfg.columns = col_ls\n",
    "\n",
    "# Verifico i risultati dell'operazione\n",
    "print (col_ls)\n",
    "print ('the shape is:',dfg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from statsmodels datasets plot best fit distribution\n",
    "\n",
    "#data = pd.Series(sm.datasets.elnino.load_pandas().data.set_index('YEAR').values.ravel())\n",
    "\n",
    "dfg_1 = dfg.copy() # .copy() avoid regressive modifications\n",
    "\n",
    "n_cols = dfg_1.shape[1]\n",
    "\n",
    "for e in np.arange(0,n_cols,1):\n",
    "\n",
    "    data = dfg_1.iloc[:,e].dropna().reset_index().copy()\n",
    "    data =pd.Series(data.values.ravel())\n",
    "\n",
    "\n",
    "    # Plot for comparison\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = data.plot(kind='hist', bins=10, normed=True, alpha=0.5)\n",
    "    # Save plot limits\n",
    "    dataYLim = ax.get_ylim()\n",
    "\n",
    "    # Find best fit distribution\n",
    "    best_fit_name, best_fit_params = best_fit_distribution(data, 200, ax)\n",
    "    best_dist = getattr(st, best_fit_name)\n",
    "\n",
    "    # Update plots\n",
    "    ax.set_ylim(dataYLim)\n",
    "    ax.set_title(dfg_1.columns[e] + u'\\n All Fitted Distributions')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "    # Make PDF with best params \n",
    "    pdf = make_pdf(best_dist, best_fit_params)\n",
    "\n",
    "    # Display\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = pdf.plot(lw=2, label='PDF', legend=True)\n",
    "    data.plot(kind='hist', bins=10, normed=True, alpha=0.5, label='Data', legend=True, ax=ax)\n",
    "\n",
    "    param_names = (best_dist.shapes + ', loc, scale').split(', ') if best_dist.shapes else ['loc', 'scale']\n",
    "    param_str = ', '.join(['{}={:0.2f}'.format(k,v) for k,v in zip(param_names, best_fit_params)])\n",
    "    dist_str = '{}({})'.format(best_fit_name, param_str)\n",
    "\n",
    "    ax.set_title(dfg_1.columns[e] + u'\\n' + dist_str)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotto il grafico multiplo di correlazione\n",
    "\n",
    "corrgraph(dfg, 'corr_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f \"^nomevaribile$\"  # cancella la variabile di come nomevariabile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
